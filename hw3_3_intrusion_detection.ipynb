{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Intrusion Detection using Two-stage Ensemble Learning\n",
    "\n",
    "Welcome to this homework - This homework implements the two-stage ensemble learning algorithms for intrusion detection that describes in the paper \"Dolus: Cyber Defense using Pretense against DDoS Attacks in Cloud Platforms\".\n",
    "\n",
    "Intrusion detection is a software application or a system to detect networking malicious activity or policy violations. In this task, you will work on the open dataset \"KDD Cup 1999 Data\" to build a network intrusion detector, a predictive model capable of distinguishing between \"bad\" connections, called intrusions, attacks, or malicious, and \"good\" normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment. \n",
    "\n",
    "**In this homework, you will learn how to:**\n",
    "- Learn the basic concept of ensemble learning and ensemble learning implementation using XGBoost\n",
    "- Learn the idea of two-stage ensemble learning for intrusion detection\n",
    "- Implement the two-stage ensemble learning using XGBoost\n",
    "\n",
    "**References**\n",
    "\n",
    "[ICDCN18] Neupane, Roshan Lal, et al. \"Dolus: cyber defense using pretense against DDoS attacks in cloud platforms.\" Proceedings of the 19th International Conference on Distributed Computing and Networking. 2018.(http://faculty.missouri.edu/calyamp/publications/dolus-cyber-defense-icdcn18.pdf)\n",
    "\n",
    "[KDD2016] Chen, Tianqi, and Carlos Guestrin. \"Xgboost: A scalable tree boosting system.\" Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "let's first import all the packages that you will need during this homework.\n",
    "- [python(3.8)](https://www.python.org/) \n",
    "- [numpy(1.19.1)](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib(3.3.0)](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [scikit-learn(0.23.2)](https://scikit-learn.org/) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "- [XGBoost(1.1.1)](https://xgboost.readthedocs.io/en/latest/) is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala.\n",
    "- [pandas(1.1.0)](https://pandas.pydata.org/) is an open-source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- [seaborn(0.10.1)](https://seaborn.pydata.org/) is an open-source, statistical data visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\alasd\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alasd\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\alasd\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alasd\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\alasd\\appdata\\roaming\\python\\python312\\site-packages (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy matplotlib pandas scikit-learn xgboost --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\alasd\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alasd\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\alasd\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alasd\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\alasd\\appdata\\roaming\\python\\python312\\site-packages (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alasd\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy matplotlib pandas scikit-learn xgboost --user \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd \n",
    "import platform\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, LabelBinarizer, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "# if platform.system() == \"Darwin\":\n",
    "#     matplotlib.use('TkAgg')\n",
    "# else:\n",
    "#     matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Explore the dataset ##\n",
    "\n",
    "We can download the dataset from the official [websites](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). In this lab, we have download all the datasets and put them into kddcup_dataset/ folder. The dataset provides a full version and a $10\\%$ version. But, we found the In this lab $10\\%$ version is not balanced well. We provide another small version with better balanced labels to demonstrate the basic idea effectively. You can visit the KDD cup 1999 original [task descrption](http://kdd.ics.uci.edu/databases/kddcup99/task.html).\n",
    "\n",
    "In short, the data used in this task is the networking packets collected from seven weeks of network traffic. Our task is to identify which packets are \"normal\" packet? and which packets are \"attack\" packet. If it's attack, we need to classify them into these four categories,\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R: unauthorized access to local superuser (root) privileges, e.g., various, \"buffer overflow\" attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "For any machine learning question, we need to understand its features and labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Features\n",
    "\n",
    "First, we can load the dataset into pandas dataframe use pandas.read_csv() interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.02</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1         2     3   4   5   6   7   8   9   ...  32    33    34    35  \\\n",
       "0   0  tcp  ftp_data   REJ   0   0   0   0   0   0  ...  62  0.27  0.02  0.01   \n",
       "1   1  tcp  ftp_data  RSTR   0   0   0   0   0   0  ...  62  0.27  0.02  0.01   \n",
       "2   0  tcp   private   REJ   0   0   0   0   0   0  ...   1  0.01  0.03  0.02   \n",
       "3   1  tcp   private  RSTR   0   0   0   0   0   0  ...   2  0.01  0.03  0.02   \n",
       "4   0  tcp   private   REJ   0   0   0   0   0   0  ...   1  0.01  0.03  0.03   \n",
       "\n",
       "     36    37   38    39    40          41  \n",
       "0  0.03  0.01  0.0  0.29  0.02  portsweep.  \n",
       "1  0.03  0.01  0.0  0.30  0.03  portsweep.  \n",
       "2  0.00  0.01  0.0  0.30  1.00  portsweep.  \n",
       "3  0.00  0.01  0.0  0.30  1.00  portsweep.  \n",
       "4  0.00  0.01  0.0  0.31  1.00  portsweep.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kddcup_dataset/kddcup.data.small', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, each packet has 41 features and one label (\"label2\") that indicates the class/type of packet. We can add the features name to the dataframe. Each feature is defined in file kddcup_dataset/kddcup.names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kddcup_dataset/kddcup.data.small', header=None)\n",
    "data.head()\n",
    "data.columns = [\n",
    "    'duration',      # continuous, length (number of seconds) of the connection \n",
    "    'protocol_type', # discrete, type of the protocol, e.g. tcp, udp, etc. \n",
    "    'service',       # discrete, network service on the destination, e.g., http, telnet, etc. \n",
    "    'flag',          # discrete, normal or error status of the connection \n",
    "    'src_bytes',     # continuous, number of data bytes from source to destination \n",
    "    'dst_bytes',     # continuous, number of data bytes from destination to source \n",
    "    'land',          # discrete, 1 if connection is from/to the same host/port; 0 otherwise \n",
    "    'wrong_fragment',# continuous, number of ``wrong'' fragments \n",
    "    'urgent',        # continuous, number of urgent packets \n",
    "    'hot',           # continuous, number of ``hot'' indicators\n",
    "    'num_failed_logins',   # continuous, number of failed login attempts \n",
    "    'logged_in',           # discrete, 1 if successfully logged in; 0 otherwise \n",
    "    'num_compromised',     # continuous, number of ``compromised'' conditions \n",
    "    'root_shell',          # discrete, 1 if root shell is obtained; 0 otherwise\n",
    "    'su_attempted',        # discrete, 1 if ``su root'' command attempted; 0 otherwise\n",
    "    'num_root',            # continuous, number of ``root'' accesses \n",
    "    'num_file_creations',  # continuous, number of file creation operations\n",
    "    'num_shells',          # continuous, number of shell prompts\n",
    "    'num_access_files',    # continuous, number of operations on access control files\n",
    "    'num_outbound_cmds',   # continuous, number of outbound commands in an ftp session \n",
    "    'is_host_login',       # discrete, 1 if the login belongs to the ``host'' list; 0 otherwise \n",
    "    'is_guest_login',      # discrete, 1 if the login is a ``guest''login; 0 otherwise \n",
    "    'count',               # continuous, number of connections to the same host as the current connection in the past two seconds \n",
    "    'srv_count',           # continuous, number of connections to the same service as the current connection in the past two seconds \n",
    "    'serror_rate',         # continuous, % of connections that have ``SYN'' errors \n",
    "    'srv_serror_rate',     # continuous, % of connections that have ``SYN'' error\n",
    "    'rerror_rate',         # continuous, % of connections that have ``REJ'' errors \n",
    "    'srv_rerror_rate',     # continuous, % of connections that have ``REJ'' errors\n",
    "    'same_srv_rate',       # continuous, % of connections to the same service \n",
    "    'diff_srv_rate',       # continuous, % of connections to different services\n",
    "    'srv_diff_host_rate',  # continuous, % of connections to different hosts\n",
    "    'dst_host_count',      # continuous, number of connections to the same host as the current connection in the past two seconds \n",
    "    'dst_host_srv_count',  # continuous, number of connections to the same service as the current connection in the past two seconds \n",
    "    'dst_host_same_srv_rate', # continuous % of connections to the same service \n",
    "    'dst_host_diff_srv_rate', # continuous, % of connections to different services \n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', \n",
    "    'dst_host_serror_rate', \n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'label2'                      # discrete, class of this packet\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.02</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>portsweep.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service  flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   REJ          0          0     0   \n",
       "1         1           tcp  ftp_data  RSTR          0          0     0   \n",
       "2         0           tcp   private   REJ          0          0     0   \n",
       "3         1           tcp   private  RSTR          0          0     0   \n",
       "4         0           tcp   private   REJ          0          0     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                  62   \n",
       "1               0       0    0  ...                  62   \n",
       "2               0       0    0  ...                   1   \n",
       "3               0       0    0  ...                   2   \n",
       "4               0       0    0  ...                   1   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.27                    0.02   \n",
       "1                    0.27                    0.02   \n",
       "2                    0.01                    0.03   \n",
       "3                    0.01                    0.03   \n",
       "4                    0.01                    0.03   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.01                         0.03   \n",
       "1                         0.01                         0.03   \n",
       "2                         0.02                         0.00   \n",
       "3                         0.02                         0.00   \n",
       "4                         0.03                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.01                       0.0                  0.29   \n",
       "1                  0.01                       0.0                  0.30   \n",
       "2                  0.01                       0.0                  0.30   \n",
       "3                  0.01                       0.0                  0.30   \n",
       "4                  0.01                       0.0                  0.31   \n",
       "\n",
       "   dst_host_srv_rerror_rate      label2  \n",
       "0                      0.02  portsweep.  \n",
       "1                      0.03  portsweep.  \n",
       "2                      1.00  portsweep.  \n",
       "3                      1.00  portsweep.  \n",
       "4                      1.00  portsweep.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the data format after adding the feature name\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Labels\n",
    "\n",
    "The label type is defined in file \"kddcup_dataset/training_attack_types\", The orginal dataset has 22 attack types, and we need to map 22 types into 4 types (dos, u2r, r2l, probe). In our task, we just need to classify each attack into four categories. \n",
    "\n",
    "We create this mapping table. We also add the \"normal\" type. Then, there are five types. So, our final task is to classify each packet into these five categories. \n",
    "\n",
    "We can check it by,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label2\n",
       "normal.             83382\n",
       "smurf.              29707\n",
       "satan.              15892\n",
       "ipsweep.            12481\n",
       "neptune.            11347\n",
       "portsweep.          10413\n",
       "nmap.                2316\n",
       "warezclient.         1020\n",
       "guess_passwd.          53\n",
       "back.                  30\n",
       "buffer_overflow.       30\n",
       "warezmaster.           20\n",
       "teardrop.              15\n",
       "imap.                  12\n",
       "rootkit.               10\n",
       "loadmodule.             9\n",
       "ftp_write.              8\n",
       "multihop.               7\n",
       "phf.                    4\n",
       "pod.                    3\n",
       "perl.                   3\n",
       "spy.                    2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_class = {\n",
    "    'normal': 'normal',\n",
    "    'back': 'dos',\n",
    "    'buffer_overflow': 'u2r',\n",
    "    'ftp_write': 'r2l',\n",
    "    'guess_passwd': 'r2l',\n",
    "    'imap': 'r2l',\n",
    "    'ipsweep': 'probe',\n",
    "    'land': 'dos',\n",
    "    'loadmodule': 'u2r',\n",
    "    'multihop': 'r2l',\n",
    "    'neptune': 'dos',\n",
    "    'nmap': 'probe',\n",
    "    'perl': 'u2r',\n",
    "    'phf': 'r2l',\n",
    "    'pod': 'dos',\n",
    "    'portsweep': 'probe',\n",
    "    'rootkit': 'u2r',\n",
    "    'satan': 'probe',\n",
    "    'smurf': 'dos',\n",
    "    'spy': 'r2l',\n",
    "    'teardrop': 'dos',\n",
    "    'warezclient': 'r2l',\n",
    "    'warezmaster': 'r2l',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use map function to map label into these five categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label2\n",
       "normal    83382\n",
       "probe     41102\n",
       "dos       41102\n",
       "r2l        1126\n",
       "u2r          52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label2'] = data.label2.apply(lambda t: packet_class[t[:-1]])\n",
    "data.label2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in this homework, we will demonstrate the idea of two-stage ensemble learning. We add another label-\"label1\" to indicate whehter it's normal packet or attack packet, so it's binary class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_class2 = {\n",
    "    'normal': 'normal',\n",
    "    'dos': 'attack',\n",
    "    'u2r': 'attack',\n",
    "    'r2l': 'attack',\n",
    "    'probe': 'attack',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label1\n",
       "attack    83382\n",
       "normal    83382\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label1'] = data.label2.apply(lambda t: packet_class2[t])\n",
    "data.label1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>label2</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.02</td>\n",
       "      <td>probe</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>probe</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>probe</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>RSTR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>probe</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>REJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>probe</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service  flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   REJ          0          0     0   \n",
       "1         1           tcp  ftp_data  RSTR          0          0     0   \n",
       "2         0           tcp   private   REJ          0          0     0   \n",
       "3         1           tcp   private  RSTR          0          0     0   \n",
       "4         0           tcp   private   REJ          0          0     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
       "0               0       0    0  ...                    0.27   \n",
       "1               0       0    0  ...                    0.27   \n",
       "2               0       0    0  ...                    0.01   \n",
       "3               0       0    0  ...                    0.01   \n",
       "4               0       0    0  ...                    0.01   \n",
       "\n",
       "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
       "0                    0.02                         0.01   \n",
       "1                    0.02                         0.01   \n",
       "2                    0.03                         0.02   \n",
       "3                    0.03                         0.02   \n",
       "4                    0.03                         0.03   \n",
       "\n",
       "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
       "0                         0.03                  0.01   \n",
       "1                         0.03                  0.01   \n",
       "2                         0.00                  0.01   \n",
       "3                         0.00                  0.01   \n",
       "4                         0.00                  0.01   \n",
       "\n",
       "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
       "0                       0.0                  0.29                      0.02   \n",
       "1                       0.0                  0.30                      0.03   \n",
       "2                       0.0                  0.30                      1.00   \n",
       "3                       0.0                  0.30                      1.00   \n",
       "4                       0.0                  0.31                      1.00   \n",
       "\n",
       "   label2  label1  \n",
       "0   probe  attack  \n",
       "1   probe  attack  \n",
       "2   probe  attack  \n",
       "3   probe  attack  \n",
       "4   probe  attack  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can observe that there are two labels - \"label1\" and \"label2\" in the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Explore features\n",
    "\n",
    "We can explore the features to understand their distributions and relevant patterns, which can help us for feature selections. Here, we demonstrate some commons methods to explore the features. You may also try other methods as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(data, features):\n",
    "    data[features].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Counts by protocol type\")\n",
    "plot_bar(data, 'protocol_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Counts by service\")\n",
    "plot_bar(data, 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Counts by flag\")\n",
    "plot_bar(data, 'flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the correlation among different features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tcp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df_corr \u001b[38;5;241m=\u001b[39m df_corr[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_corr \u001b[38;5;28;01mif\u001b[39;00m df_corr[col]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m----> 3\u001b[0m corr \u001b[38;5;241m=\u001b[39m df_corr\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(corr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11049\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  11047\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  11048\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 11049\u001b[0m mat \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  11051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  11052\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1993\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1992\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1993\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(result, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1753\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1753\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1754\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tcp'"
     ]
    }
   ],
   "source": [
    "df_corr = data.dropna(axis='columns')\n",
    "df_corr = df_corr[[col for col in df_corr if df_corr[col].nunique() > 1]]\n",
    "corr = df_corr.corr()\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Features correlations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure above, the lighter color indicates a higher correlation between two features, and redundant features can be eliminated by higher correlations. Although current advanced machine learning models like deep neural network (DNN) can learn useful features by themselves, it's also very useful to understand the basic theory of feature selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Ensemble Learning ##\n",
    "\n",
    "In this section, we will briefly introduce the concept of ensemble learning. In short, Ensemble learning uses multiple learning models to imporve the predictive performance than a single learning model. Common ensemble learning methods are bagging, boosting, and stacking. \n",
    "\n",
    "* __Bagging__: Bootstrap Aggregation or Bagging has two distinct features which define its training and prediction. For training it leverages a Bootstrap procedure to separate the training data into different random subsamples, which different iterations of the model use to train on. For prediction, a bagging classifier will use the prediction with the most votes from each model to produce its output and a bagging regression will take an average of all models to produce an output. Bagging is typically applied to high variance models such as Decision Trees and the Random Forest algorithm is a very close variation on bagging.\n",
    "\n",
    "* __Boosting__: The core definition of boosting is a method that converts weak learners to strong learners, and is typically applied to trees. More explicitly, a boosting algorithm adds iterations of the model sequentially, adjusting the weights of the weak-learners along the way. This reduces bias from the model and typically improves accuracy. Popular boosting algorithms are AdaBoost, Gradient Tree Boosting, and XGBoost, which we’ll focus on here.\n",
    "\n",
    "* __Stacking__: A Stacking model is a “meta-model” which leverages the outputs from a collection of many, typically significantly different, models as input features. For instance, this allows you to train a K-NN, Linear Regression, and Decision Tree with all of your training data, then take those outputs and merge them with a Logistical Regression. The idea is that this can reduce overfitting and improve accuracy.\n",
    "\n",
    "In this lab, we foucus on the Boosting methods using XGBoost. XGBoost stands for “Extreme Gradient Boosting” that is based on gradient boosting algorithm. In this Lab, you will learn to how to use XGBoost for attack detection (i.e., classificaiton tasks). More information about XGBoost, please visit visited XGBoost paper [KDD2016](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf) and its [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Two-stage Ensemble Learning\n",
    "\n",
    "The two-stage ensemble learning method is described in the paper [ICDCN18](http://faculty.missouri.edu/calyamp/publications/dolus-cyber-defense-icdcn18.pdf). \n",
    "\n",
    "The two-stage ensemble learning has two stages: \n",
    "* __Detection Stage__: detect whether a packet is an attack packet or a normal packet. It's a binary classification task ('normal', 'attack')\n",
    "* __Classification Stage__: classify each attack into these four categories ('dos', 'u2r', 'r2l', 'probe')\n",
    "\n",
    "As we know, the first stage is a binary classification task and the second stage is multi-classes classifications task. Hence, the task in the first stage is easier than the task in the second stage. Theoretically, we can use fewer features and a simpler model in stage 1 than stage 2. \n",
    "\n",
    "####  Why do we use two-stage ensemble learning?\n",
    "\n",
    "In fact, we can just use one model to classify each packet into 5 classes ('normal', 'dos', 'u2r', 'r2l', 'probe') in one stage. The reason for proposing the two-stage ensemble learning is for computational efficiency. \n",
    "\n",
    "In the real-time intrusion detection system (IDS), the packets collectors are distributed around the different hosts, and the collectors need to send the collected packets to the central server to detect attacks. Because the ratio of attack packets is much smaller than the normal packets, most of the time of computation resources and networking transmission resources are wasted in processing \"normal packet\", which may also cause real attack packets cannot be processed in real-time. \n",
    "\n",
    "Our two-stage ensemble learning is to deal with these issues. Because we consider that detection task (binary classification) is simpler than the classification task (multi-classes) classification. For the easier task, we can use less information (or features) to train a model and detect using a fewer amount of dataset; and for the harder task, we can use more information to train and test. \n",
    "\n",
    "The whole two-stage ensemble learning can be divided into two phrases,\n",
    "\n",
    "\n",
    "#### Training phrase\n",
    "During the trainging phrase, we train two model separately: one is __detection model__ to detect attack or not; one is __classification model__ to classify attacks. In the first task, we need to use as little features and simpler as possible at situations of similar performance. \n",
    "\n",
    "#### Testing phrase\n",
    "During the testing phrase, we always use __detection model__  with less features datasets to detect attack in the beginning. When we detect there is an attack. The server will to request packet client to ask for more features datasets, then we will use __classification model__ to classify attacks. \n",
    "\n",
    "In our lab, we only foucs on the machine learning model. And, when to shifing from __deteciton model__ to __classification model__ and when to make a request for more features dataset belongs to system level, which is out of scope of this lab.\n",
    "\n",
    "Next section, we will implement the two-stage ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Two-stage ensemble learning implementaion\n",
    "\n",
    "In the section, we will implement the two-stage ensemble learning method\n",
    "\n",
    "### 5.1 - Detection Model\n",
    "\n",
    "For detection model, we use \"label1\" (normal, attack) for binary classification,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Prepare training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding string features into integer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].dtype == object and column != 'label1' and column != 'label2':\n",
    "        encoded = LabelEncoder()\n",
    "        \n",
    "        encoded.fit(data[column])\n",
    "        data[column] = encoded.transform(data[column])\n",
    "\n",
    "# Encoding label into integer \n",
    "label1_encoded = LabelEncoder()\n",
    "label1_encoded.fit(data['label1'])\n",
    "data['label1'] = label1_encoded.transform(data['label1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare inputs $X$ and labels $Y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our X, which drop label1 and label 2\n",
    "X = data.drop(['label1', 'label2'], axis=1)\n",
    "Y = data.label1\n",
    "\n",
    "# Split our train and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "        \n",
    "# We can also rescale some features\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train[['dst_bytes','src_bytes']] = scaler.fit_transform(X_train[['dst_bytes','src_bytes']])\n",
    "X_test[['dst_bytes','src_bytes']] = scaler.transform(X_test[['dst_bytes','src_bytes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Select features\n",
    "\n",
    "At the situation of similar performance, select as fewer features as possible,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['protocol_type', 'src_bytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "detector = XGBClassifier(max_depth=3, n_estimators=10, random_state=42, verbosity=1)\n",
    "\n",
    "# train the model\n",
    "detector.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Model testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(\"The trianing accuracy: \", detector.score(X_train[selected_features], y_train))\n",
    "print(\"The testing accuracy: \",detector.score(X_test[selected_features],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can achieve $96\\%$ accuracy in test datasets with only 2 features that effectively saves computational and networking resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Classification Model\n",
    "\n",
    "For classification model, our task is to classify each attack into four categories ('dos', 'u2r', 'r2l', 'probe'). Hence, we just need to consider the attack dataset, Hence, we need to drop \"normal\" labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Prepare training and testing datasets\n",
    "\n",
    "We need to drop the rows with label \"normal\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = data[data.label2 != 'normal']\n",
    "data_cls.label2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding label into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2_encoded = LabelEncoder()\n",
    "label2_encoded.fit(data_cls['label2'])\n",
    "data_cls['label2'] = label2_encoded.transform(data_cls['label2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare inputs $X$ and labels $Y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our X, which drop label1 and label 2\n",
    "X = data_cls.drop(['label1', 'label2'], axis=1)\n",
    "Y = data_cls.label2\n",
    "\n",
    "# Split our train and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "        \n",
    "# We can also rescale some features\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train[['dst_bytes','src_bytes']] = scaler.fit_transform(X_train[['dst_bytes','src_bytes']])\n",
    "X_test[['dst_bytes','src_bytes']] = scaler.transform(X_test[['dst_bytes','src_bytes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Select features\n",
    "\n",
    "We can just use same features as detector model to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a wider range of features\n",
    "selected_features = ['protocol_type', 'src_bytes', 'num_failed_logins', 'hot', 'num_compromised', \n",
    "                     'num_file_creations', 'count', 'srv_count', 'serror_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "classifier = XGBClassifier(max_depth=3, n_estimators=10, random_state=42, verbosity=1)\n",
    "\n",
    "# train the model\n",
    "classifier.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "print(\"The training accuracy: \", classifier.score(X_train[selected_features], y_train))\n",
    "print(\"The testing accuracy: \",classifier.score(X_test[selected_features], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the confusion matrix to check performance for each class,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test[selected_features])\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Conclusion\n",
    "\n",
    "Hence, if we use same features in detector and classification model, the detector model (simple model) works better than classification model. If we want to imporve the performance of classification model, we need to add more features or increase the complexity of the model. You need to implement this in your homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Explain the main purposes of two-stages ensemble learning?\n",
    "2. As we mention in this lab, the original $10\\%$ dataset is not balanced well. Can you explain the effect of imbalanced datasets? and how to deal with imbalanced datasets?\n",
    "3. Take research about XGBoost, can you briefly introduce what is XGBoost and differences between XGBoost and GBDT?\n",
    "4. Using lab results in section 5 as our baseline, can you improve the testing accuracy performance of the classification model by adding more features or complexity of ensemble models or any other methods. But, the model should be restricted in XGBoost. Explain how you improve the performance (e.g., the features you used, parameters you adjusted).  Please compyu submit your executable jupyter notebook file (*.ipynb) with output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
